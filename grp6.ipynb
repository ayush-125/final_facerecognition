{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.41960785 0.627451   0.7490196 ]\n",
      "   [0.41568628 0.627451   0.75686276]\n",
      "   [0.39607844 0.6117647  0.7529412 ]\n",
      "   ...\n",
      "   [0.6745098  0.8392157  0.8666667 ]\n",
      "   [0.6784314  0.8352941  0.8666667 ]\n",
      "   [0.6745098  0.81960785 0.8509804 ]]\n",
      "\n",
      "  [[0.4117647  0.6392157  0.8       ]\n",
      "   [0.38039216 0.6156863  0.76862746]\n",
      "   [0.3882353  0.6117647  0.7529412 ]\n",
      "   ...\n",
      "   [0.68235296 0.84705883 0.8784314 ]\n",
      "   [0.68235296 0.8392157  0.87058824]\n",
      "   [0.6745098  0.81960785 0.8509804 ]]\n",
      "\n",
      "  [[0.38039216 0.60784316 0.78431374]\n",
      "   [0.3137255  0.54901963 0.70980394]\n",
      "   [0.33333334 0.5568628  0.69803923]\n",
      "   ...\n",
      "   [0.6745098  0.84313726 0.87058824]\n",
      "   [0.68235296 0.8392157  0.87058824]\n",
      "   [0.6862745  0.83137256 0.8627451 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.12156863 0.07450981 0.10980392]\n",
      "   [0.13333334 0.09803922 0.13725491]\n",
      "   [0.14509805 0.10588235 0.15294118]\n",
      "   ...\n",
      "   [0.7058824  0.79607844 0.827451  ]\n",
      "   [0.7019608  0.7921569  0.8235294 ]\n",
      "   [0.69411767 0.78431374 0.8156863 ]]\n",
      "\n",
      "  [[0.1254902  0.08235294 0.11764706]\n",
      "   [0.11764706 0.08235294 0.1254902 ]\n",
      "   [0.13333334 0.09411765 0.13725491]\n",
      "   ...\n",
      "   [0.7019608  0.78431374 0.81960785]\n",
      "   [0.69411767 0.78431374 0.8156863 ]\n",
      "   [0.69411767 0.78431374 0.8156863 ]]\n",
      "\n",
      "  [[0.11764706 0.07450981 0.10980392]\n",
      "   [0.1254902  0.09019608 0.13333334]\n",
      "   [0.13333334 0.09411765 0.13725491]\n",
      "   ...\n",
      "   [0.7019608  0.7921569  0.8235294 ]\n",
      "   [0.69803923 0.7882353  0.81960785]\n",
      "   [0.69411767 0.78431374 0.8156863 ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.20392157 0.16078432 0.10588235]\n",
      "   [0.22745098 0.17254902 0.1254902 ]\n",
      "   [0.25490198 0.19215687 0.14509805]\n",
      "   ...\n",
      "   [0.14509805 0.10980392 0.07450981]\n",
      "   [0.09411765 0.06666667 0.02745098]\n",
      "   [0.07450981 0.04705882 0.01568628]]\n",
      "\n",
      "  [[0.2        0.15686275 0.10196079]\n",
      "   [0.22352941 0.17254902 0.1254902 ]\n",
      "   [0.23529412 0.16862746 0.12156863]\n",
      "   ...\n",
      "   [0.10588235 0.07450981 0.03921569]\n",
      "   [0.08627451 0.05882353 0.02352941]\n",
      "   [0.0627451  0.03137255 0.00392157]]\n",
      "\n",
      "  [[0.19215687 0.15686275 0.10196079]\n",
      "   [0.22352941 0.17254902 0.1254902 ]\n",
      "   [0.23921569 0.17254902 0.1254902 ]\n",
      "   ...\n",
      "   [0.10588235 0.07058824 0.03921569]\n",
      "   [0.10196079 0.0627451  0.03529412]\n",
      "   [0.07058824 0.02745098 0.00392157]]]\n",
      "\n",
      "\n",
      " [[[0.         0.00392157 0.        ]\n",
      "   [0.         0.00392157 0.        ]\n",
      "   [0.         0.00392157 0.        ]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00784314]\n",
      "   [0.         0.00392157 0.00392157]\n",
      "   [0.         0.00392157 0.00784314]]\n",
      "\n",
      "  [[0.4        0.48235294 0.47058824]\n",
      "   [0.40392157 0.48235294 0.4745098 ]\n",
      "   [0.40392157 0.4862745  0.47843137]\n",
      "   ...\n",
      "   [0.5176471  0.5921569  0.59607846]\n",
      "   [0.5176471  0.5921569  0.6       ]\n",
      "   [0.52156866 0.59607846 0.6039216 ]]\n",
      "\n",
      "  [[0.39215687 0.4745098  0.46666667]\n",
      "   [0.39607844 0.47843137 0.47058824]\n",
      "   [0.4        0.48235294 0.4745098 ]\n",
      "   ...\n",
      "   [0.52156866 0.59607846 0.60784316]\n",
      "   [0.5176471  0.5921569  0.6039216 ]\n",
      "   [0.5137255  0.5882353  0.6       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.04313726 0.03137255 0.03921569]\n",
      "   [0.05098039 0.03921569 0.04705882]\n",
      "   [0.05490196 0.04313726 0.05098039]\n",
      "   ...\n",
      "   [0.12156863 0.09411765 0.09803922]\n",
      "   [0.11372549 0.09411765 0.09803922]\n",
      "   [0.09019608 0.09019608 0.09803922]]\n",
      "\n",
      "  [[0.07450981 0.0627451  0.07058824]\n",
      "   [0.05098039 0.03921569 0.04705882]\n",
      "   [0.05490196 0.04313726 0.05098039]\n",
      "   ...\n",
      "   [0.11372549 0.09411765 0.09803922]\n",
      "   [0.10588235 0.08627451 0.09019608]\n",
      "   [0.10196079 0.09411765 0.10196079]]\n",
      "\n",
      "  [[0.02352941 0.01176471 0.01960784]\n",
      "   [0.07843138 0.0627451  0.07058824]\n",
      "   [0.07450981 0.0627451  0.07058824]\n",
      "   ...\n",
      "   [0.12156863 0.10196079 0.10196079]\n",
      "   [0.10980392 0.09411765 0.09411765]\n",
      "   [0.10588235 0.09803922 0.09803922]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.45490196 0.59607846 0.7490196 ]\n",
      "   [0.5058824  0.64705884 0.7607843 ]\n",
      "   [0.54901963 0.6666667  0.7490196 ]\n",
      "   ...\n",
      "   [0.28235295 0.24705882 0.19607843]\n",
      "   [0.28235295 0.24705882 0.19607843]\n",
      "   [0.28235295 0.24705882 0.19607843]]\n",
      "\n",
      "  [[0.5019608  0.6392157  0.7647059 ]\n",
      "   [0.4392157  0.56078434 0.654902  ]\n",
      "   [0.25882354 0.38039216 0.4745098 ]\n",
      "   ...\n",
      "   [0.28235295 0.24705882 0.19607843]\n",
      "   [0.28235295 0.24705882 0.19607843]\n",
      "   [0.28235295 0.24705882 0.19607843]]\n",
      "\n",
      "  [[0.5019608  0.62352943 0.70980394]\n",
      "   [0.37254903 0.49019608 0.5647059 ]\n",
      "   [0.26666668 0.38431373 0.4862745 ]\n",
      "   ...\n",
      "   [0.28235295 0.24705882 0.19607843]\n",
      "   [0.28235295 0.24705882 0.19607843]\n",
      "   [0.28235295 0.24705882 0.19607843]]]\n",
      "\n",
      "\n",
      " [[[0.00392157 0.         0.        ]\n",
      "   [0.00392157 0.         0.        ]\n",
      "   [0.00392157 0.         0.        ]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.        ]\n",
      "   [0.00392157 0.00392157 0.        ]\n",
      "   [0.00392157 0.00392157 0.        ]]\n",
      "\n",
      "  [[0.08627451 0.09803922 0.10196079]\n",
      "   [0.08627451 0.09411765 0.10196079]\n",
      "   [0.09019608 0.09411765 0.10196079]\n",
      "   ...\n",
      "   [0.11372549 0.12941177 0.13333334]\n",
      "   [0.11372549 0.12941177 0.13333334]\n",
      "   [0.11764706 0.13333334 0.13725491]]\n",
      "\n",
      "  [[0.42745098 0.4745098  0.48235294]\n",
      "   [0.43137255 0.47058824 0.48235294]\n",
      "   [0.43137255 0.45882353 0.47058824]\n",
      "   ...\n",
      "   [0.49803922 0.58431375 0.5882353 ]\n",
      "   [0.5058824  0.5921569  0.59607846]\n",
      "   [0.5058824  0.5882353  0.5921569 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.18039216 0.16470589 0.14117648]\n",
      "   [0.21568628 0.19215687 0.16078432]\n",
      "   [0.23529412 0.20392157 0.16470589]\n",
      "   ...\n",
      "   [0.20392157 0.18039216 0.13333334]\n",
      "   [0.24313726 0.20784314 0.16862746]\n",
      "   [0.2509804  0.21176471 0.17254902]]\n",
      "\n",
      "  [[0.16470589 0.14901961 0.13725491]\n",
      "   [0.19215687 0.1764706  0.14509805]\n",
      "   [0.23921569 0.20392157 0.1764706 ]\n",
      "   ...\n",
      "   [0.19607843 0.1764706  0.14117648]\n",
      "   [0.23529412 0.20784314 0.17254902]\n",
      "   [0.21960784 0.1882353  0.15294118]]\n",
      "\n",
      "  [[0.15686275 0.14117648 0.14509805]\n",
      "   [0.1764706  0.14509805 0.13333334]\n",
      "   [0.23137255 0.19215687 0.1764706 ]\n",
      "   ...\n",
      "   [0.21176471 0.19607843 0.16862746]\n",
      "   [0.20392157 0.18039216 0.15294118]\n",
      "   [0.18431373 0.16078432 0.13333334]]]]\n",
      "['Ariel_Sharon' 'Ariel_Sharon' 'Ariel_Sharon' 'Ariel_Sharon'\n",
      " 'Ariel_Sharon']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def load_and_preprocess_dataset(dataset_path, target_size=(64, 64)):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for dirs in os.listdir(dataset_path):\n",
    "        \n",
    "        for dir in dirs:\n",
    "            \n",
    "            a=os.path.join(dataset_path, dir)\n",
    "            \n",
    "            for root1,dirs1,files1 in os.walk(a):\n",
    "                for file in files1:\n",
    "                    \n",
    "                    if file.endswith(\".jpg\"):\n",
    "                        image_path = os.path.join(a, file)\n",
    "                        label = dir\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        # Load and preprocess the image\n",
    "                        img = cv2.imread(image_path)\n",
    "                        img = cv2.resize(img, target_size)\n",
    "                        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "                        image_data.append(img)\n",
    "                        labels.append(label)\n",
    "            \n",
    "    # return 0,0\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "dataset_path =  r'C:\\Users\\HP\\Downloads\\lfw\\train'\n",
    "x_train, y_train = load_and_preprocess_dataset(dataset_path)\n",
    "print (x_train[0:5])\n",
    "print (y_train[0:5])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have loaded your x_train and y_train data from your dataset\n",
    "# If not, make sure to load them first\n",
    "\n",
    "# Get the number of samples in your training data\n",
    "num_samples = x_train.shape[0]\n",
    "\n",
    "# Create a random permutation of indices to shuffle the data\n",
    "permutation = np.random.permutation(num_samples)\n",
    "\n",
    "# Use the permutation to shuffle both x_train and y_train\n",
    "x_train = x_train[permutation]\n",
    "y_train = y_train[permutation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "# include convolutional layers\n",
    "from keras.layers import Conv2D, Dropout\n",
    "# Pooling layers\n",
    "from keras.layers import MaxPooling2D\n",
    "# flatten layers into single vector\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    scale = 8.0\n",
    "    margin = 0.20\n",
    "    t=0.8\n",
    "\n",
    "    # Convert y_true to float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # L2 normalize embeddings\n",
    "    costheta = K.l2_normalize(y_pred, axis=1)\n",
    "    theta=tf.acos(costheta)\n",
    "    # print(y_pred.shape)\n",
    "    m_theta=theta+margin*y_true\n",
    "    m_costheta=tf.cos(m_theta)\n",
    "    temp=tf.reduce_sum((y_true*m_costheta),axis=1)\n",
    "    extended_tensor = tf.expand_dims(temp, axis=1)\n",
    "    # Duplicate the original data 7 times\n",
    "    value = tf.concat([extended_tensor] * num_classes, axis=1)\n",
    "    # print(value.shape)\n",
    "\n",
    "    condition=m_costheta>value\n",
    "    f=tf.where(condition,t*(m_costheta),m_costheta)\n",
    "    # print(f.shape)\n",
    "    y_pred=scale*f\n",
    "\n",
    "    # Compute the final Cosine loss\n",
    "    loss = K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    # Combine the loss with the angular margin penalty\n",
    "    cos_loss = K.mean(loss , axis=-1)\n",
    "\n",
    "    return cos_loss\n",
    "\n",
    "\n",
    "# def cosine_loss(y_true, y_pred):\n",
    "#     scale = 30.0\n",
    "#     margin = 0.35\n",
    "\n",
    "\n",
    "#     # Convert y_true to float32\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "#     # L2 normalize embeddings\n",
    "#     costheta = K.l2_normalize(y_pred, axis=1)\n",
    "#     theta=tf.acos(costheta)\n",
    "\n",
    "#     m_theta=theta+margin*y_true\n",
    "#     m_costheta=tf.cos(m_theta)\n",
    "\n",
    "#     y_pred=scale*m_costheta\n",
    "\n",
    "#     # Compute the final Cosine loss\n",
    "#     loss = K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "#     # Combine the loss with the angular margin penalty\n",
    "#     cos_loss = K.mean(loss , axis=-1)\n",
    "\n",
    "#     return cos_loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gerhard_Schroeder' 'Ariel_Sharon' 'Colin_Powell' 'Colin_Powell'\n",
      " 'George_W_Bush' 'Tony_Blair' 'Colin_Powell' 'George_W_Bush' 'Tony_Blair'\n",
      " 'George_W_Bush']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocess the data, e.g., resize images to a common size and normalize pixel values\n",
    "x_train = [cv2.resize(image, (64, 64)) for image in x_train]\n",
    "x_val = [cv2.resize(image, (64, 64)) for image in x_val]\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(set(y_train))\n",
    "print(num_classes)\n",
    "\n",
    "x_test, y_test = x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (310, 3) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Documents\\grp6.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/grp6.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/grp6.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m label_encoder \u001b[39m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/grp6.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m y_train_encoded \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39;49mfit_transform(y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/grp6.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_train_encoded))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/grp6.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m labels_one_hot \u001b[39m=\u001b[39m to_categorical(y_train_encoded, num_classes\u001b[39m=\u001b[39mnum_classes)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m        Encoded labels.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_, y \u001b[39m=\u001b[39m _unique(y, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1244\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1233\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1234\u001b[0m             (\n\u001b[0;32m   1235\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   1241\u001b[0m         )\n\u001b[0;32m   1242\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_order(xp\u001b[39m.\u001b[39mreshape(y, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, xp\u001b[39m=\u001b[39mxp)\n\u001b[1;32m-> 1244\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1245\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[0;32m   1246\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (310, 3) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "labels_one_hot = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "\n",
    "# print(labels_one_hot)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosface_loss(y_true, y_pred):\n",
    "    scale = 30.0\n",
    "    margin = 0.35\n",
    "\n",
    "    # Convert y_true to float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # L2 normalize embeddings\n",
    "    y_pred = K.l2_normalize(y_pred, axis=1)\n",
    "\n",
    "    y_pred=scale*(y_pred-margin*y_true)\n",
    "\n",
    "    # Compute the final CosFace loss\n",
    "    loss = K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "\n",
    "    # Combine the loss with the angular margin penalty\n",
    "    cos_loss = K.mean(loss , axis=-1)\n",
    "\n",
    "    return cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class CosFaceLoss(tf.keras.losses.Loss):\n",
    "#     def __init__(self, margin=0.35, scale=30):\n",
    "#         super(CosFaceLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "#         self.scale = scale\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         y_pred = tf.nn.l2_normalize(y_pred, axis=1)\n",
    "#         cos_t = tf.reduce_sum(y_pred * y_true, axis=1)\n",
    "#         cos_m = tf.math.cos(self.margin)\n",
    "        \n",
    "#         # Calculate the margin loss only for positive samples (where y_true has a positive class)\n",
    "#         positive_samples = tf.math.reduce_any(y_true > 0, axis=1)\n",
    "#         positive_cos_t = tf.boolean_mask(cos_t, positive_samples)\n",
    "#         loss = tf.reduce_mean(tf.where(positive_samples, tf.math.acos(cos_m) - positive_cos_t, 0.0))\n",
    "        \n",
    "#         return loss * self.scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 3)\n",
      "(310, 128, 128, 3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(x_train.shape)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 8s 558ms/step - loss: 18.1440 - accuracy: 0.3742 - val_loss: 19.9062 - val_accuracy: 0.4359\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 5s 486ms/step - loss: 14.1970 - accuracy: 0.4806 - val_loss: 16.5140 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 5s 477ms/step - loss: 13.3357 - accuracy: 0.5032 - val_loss: 21.5702 - val_accuracy: 0.3718\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 5s 478ms/step - loss: 11.8205 - accuracy: 0.5645 - val_loss: 14.9528 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 10.9142 - accuracy: 0.5871 - val_loss: 13.1535 - val_accuracy: 0.5256\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 11.1274 - accuracy: 0.5677 - val_loss: 12.7449 - val_accuracy: 0.5256\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 5s 499ms/step - loss: 9.3596 - accuracy: 0.6258 - val_loss: 16.0371 - val_accuracy: 0.3974\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 5s 464ms/step - loss: 8.0691 - accuracy: 0.6806 - val_loss: 14.2506 - val_accuracy: 0.4744\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 9.0741 - accuracy: 0.6645 - val_loss: 13.7329 - val_accuracy: 0.4103\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 5s 485ms/step - loss: 7.6134 - accuracy: 0.7129 - val_loss: 16.0761 - val_accuracy: 0.3974\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 16.0761 - accuracy: 0.3974\n",
      "Test loss: 16.0761, Test accuracy: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "# Define the model architecture\n",
    "def create_model(num_classes):\n",
    "    \n",
    "    input_tensor = Input(shape=(128, 128, 3))\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(input_tensor)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(input_tensor)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('leakyrelu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_tensor, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model(num_classes=num_classes)\n",
    "model.compile(optimizer=Adam(), loss=cosface_loss, metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model and label encoder\n",
    "model.save(\"face_recognition_model.h5\")\n",
    "np.save(\"label_encoder.npy\", 5749)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 976 images belonging to 6 classes.\n",
      "Found 241 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Add validation split\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r'C:\\Users\\HP\\Downloads\\lfw\\train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # 'binary' for binary classification\n",
    "    subset='training'     # Specify subset as 'training' for training data\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    r'C:\\Users\\HP\\Downloads\\lfw\\train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # 'binary' for binary classification\n",
    "    subset='validation'   # Specify subset as 'validation' for validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[[0.02745098, 0.        , 0.        ],\n",
      "         [0.03529412, 0.        , 0.        ],\n",
      "         [0.03137255, 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.07058824],\n",
      "         [0.        , 0.        , 0.0627451 ],\n",
      "         [0.        , 0.        , 0.0627451 ]],\n",
      "\n",
      "        [[0.6039216 , 0.56078434, 0.47450984],\n",
      "         [0.58431375, 0.54509807, 0.44705886],\n",
      "         [0.5411765 , 0.49803925, 0.38823533],\n",
      "         ...,\n",
      "         [0.7960785 , 0.83921576, 0.9176471 ],\n",
      "         [0.78823537, 0.8313726 , 0.90196085],\n",
      "         [0.78823537, 0.8352942 , 0.8980393 ]],\n",
      "\n",
      "        [[0.6039216 , 0.54901963, 0.41176474],\n",
      "         [0.59607846, 0.5411765 , 0.39607847],\n",
      "         [0.5529412 , 0.49411768, 0.34117648],\n",
      "         ...,\n",
      "         [0.7843138 , 0.8470589 , 0.9450981 ],\n",
      "         [0.7843138 , 0.8431373 , 0.9333334 ],\n",
      "         [0.7803922 , 0.83921576, 0.9294118 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0627451 , 0.0509804 , 0.01568628],\n",
      "         [0.05490196, 0.04313726, 0.01568628],\n",
      "         [0.06666667, 0.05490196, 0.02745098],\n",
      "         ...,\n",
      "         [0.73333335, 0.5254902 , 0.36078432],\n",
      "         [0.7294118 , 0.52156866, 0.35686275],\n",
      "         [0.6901961 , 0.48235297, 0.31764707]],\n",
      "\n",
      "        [[0.0627451 , 0.0509804 , 0.01568628],\n",
      "         [0.0509804 , 0.03921569, 0.01176471],\n",
      "         [0.0627451 , 0.0509804 , 0.02352941],\n",
      "         ...,\n",
      "         [0.6901961 , 0.48235297, 0.31764707],\n",
      "         [0.7137255 , 0.5058824 , 0.34117648],\n",
      "         [0.7137255 , 0.5058824 , 0.33333334]],\n",
      "\n",
      "        [[0.0627451 , 0.0509804 , 0.01568628],\n",
      "         [0.05882353, 0.04705883, 0.01960784],\n",
      "         [0.06666667, 0.05490196, 0.02745098],\n",
      "         ...,\n",
      "         [0.6784314 , 0.47058827, 0.3137255 ],\n",
      "         [0.72156864, 0.5137255 , 0.34901962],\n",
      "         [0.7254902 , 0.5176471 , 0.34509805]]],\n",
      "\n",
      "\n",
      "       [[[0.9843138 , 0.96470594, 0.9803922 ],\n",
      "         [1.        , 0.9843138 , 1.        ],\n",
      "         [1.        , 0.98823535, 1.        ],\n",
      "         ...,\n",
      "         [0.22352943, 0.227451  , 0.20392159],\n",
      "         [0.25882354, 0.2627451 , 0.2392157 ],\n",
      "         [0.28235295, 0.28627452, 0.2627451 ]],\n",
      "\n",
      "        [[0.98823535, 0.9686275 , 0.9921569 ],\n",
      "         [0.9921569 , 0.9843138 , 0.9960785 ],\n",
      "         [0.9921569 , 0.9843138 , 0.9960785 ],\n",
      "         ...,\n",
      "         [0.20784315, 0.21176472, 0.18823531],\n",
      "         [0.24313727, 0.24705884, 0.22352943],\n",
      "         [0.27058825, 0.27450982, 0.2509804 ]],\n",
      "\n",
      "        [[0.9843138 , 0.97647065, 0.9960785 ],\n",
      "         [0.9921569 , 0.9843138 , 1.        ],\n",
      "         [0.98823535, 0.9803922 , 1.        ],\n",
      "         ...,\n",
      "         [0.18823531, 0.19215688, 0.17254902],\n",
      "         [0.227451  , 0.23137257, 0.20784315],\n",
      "         [0.25490198, 0.25882354, 0.23529413]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0509804 , 0.06666667, 0.07058824],\n",
      "         [0.05882353, 0.07450981, 0.07843138],\n",
      "         [0.05490196, 0.07058824, 0.07450981],\n",
      "         ...,\n",
      "         [0.3137255 , 0.33333334, 0.34509805],\n",
      "         [0.2784314 , 0.3019608 , 0.3019608 ],\n",
      "         [0.24313727, 0.26666668, 0.26666668]],\n",
      "\n",
      "        [[0.0509804 , 0.06666667, 0.07058824],\n",
      "         [0.05882353, 0.07450981, 0.07843138],\n",
      "         [0.05490196, 0.07058824, 0.07450981],\n",
      "         ...,\n",
      "         [0.3529412 , 0.37254903, 0.38431376],\n",
      "         [0.32941177, 0.3529412 , 0.3529412 ],\n",
      "         [0.29411766, 0.31764707, 0.31764707]],\n",
      "\n",
      "        [[0.0509804 , 0.06666667, 0.07058824],\n",
      "         [0.05882353, 0.07450981, 0.07843138],\n",
      "         [0.05490196, 0.07058824, 0.07450981],\n",
      "         ...,\n",
      "         [0.39607847, 0.4156863 , 0.427451  ],\n",
      "         [0.37647063, 0.40000004, 0.40000004],\n",
      "         [0.3372549 , 0.36078432, 0.36078432]]],\n",
      "\n",
      "\n",
      "       [[[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.01568628, 0.00784314, 0.02745098],\n",
      "         [0.00784314, 0.00392157, 0.02352941],\n",
      "         [0.00392157, 0.        , 0.02352941],\n",
      "         ...,\n",
      "         [0.06666667, 0.10196079, 0.12156864],\n",
      "         [0.00392157, 0.03137255, 0.07058824],\n",
      "         [0.00392157, 0.01960784, 0.06666667]],\n",
      "\n",
      "        [[0.00392157, 0.        , 0.01960784],\n",
      "         [0.00392157, 0.        , 0.01960784],\n",
      "         [0.00392157, 0.        , 0.02352941],\n",
      "         ...,\n",
      "         [0.07450981, 0.10980393, 0.14509805],\n",
      "         [0.        , 0.01960784, 0.06666667],\n",
      "         [0.        , 0.01176471, 0.07058824]],\n",
      "\n",
      "        [[0.01960784, 0.01568628, 0.03529412],\n",
      "         [0.        , 0.        , 0.01568628],\n",
      "         [0.00784314, 0.00392157, 0.02745098],\n",
      "         ...,\n",
      "         [0.0509804 , 0.08627451, 0.12156864],\n",
      "         [0.        , 0.01176471, 0.06666667],\n",
      "         [0.        , 0.01176471, 0.07843138]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.00392157, 0.        ],\n",
      "         [0.        , 0.00392157, 0.        ],\n",
      "         [0.        , 0.00392157, 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.20000002, 0.23529413, 0.23137257],\n",
      "         [0.13725491, 0.17254902, 0.16862746],\n",
      "         [0.1137255 , 0.14901961, 0.14509805],\n",
      "         ...,\n",
      "         [0.09411766, 0.1137255 , 0.1254902 ],\n",
      "         [0.09803922, 0.11764707, 0.12941177],\n",
      "         [0.10196079, 0.11764707, 0.12941177]],\n",
      "\n",
      "        [[0.18823531, 0.22352943, 0.21960786],\n",
      "         [0.13333334, 0.16862746, 0.16470589],\n",
      "         [0.11764707, 0.15294118, 0.14901961],\n",
      "         ...,\n",
      "         [0.12156864, 0.14117648, 0.15294118],\n",
      "         [0.1137255 , 0.13333334, 0.14509805],\n",
      "         [0.11764707, 0.13725491, 0.14901961]],\n",
      "\n",
      "        [[0.1764706 , 0.21176472, 0.20784315],\n",
      "         [0.12941177, 0.16470589, 0.16078432],\n",
      "         [0.10588236, 0.14117648, 0.13725491],\n",
      "         ...,\n",
      "         [0.12156864, 0.14117648, 0.15294118],\n",
      "         [0.12941177, 0.14901961, 0.16078432],\n",
      "         [0.1254902 , 0.14509805, 0.15686275]]],\n",
      "\n",
      "\n",
      "       [[[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.03921569, 0.03921569, 0.03137255],\n",
      "         [0.01960784, 0.02745098, 0.01568628],\n",
      "         [0.00784314, 0.02745098, 0.01176471],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.03137255, 0.03137255, 0.02352941],\n",
      "         [0.03137255, 0.03921569, 0.02745098],\n",
      "         [0.00392157, 0.02352941, 0.00784314],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.03137255, 0.03137255, 0.02352941],\n",
      "         [0.03921569, 0.04705883, 0.03529412],\n",
      "         [0.00392157, 0.02352941, 0.00784314],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]]],\n",
      "\n",
      "\n",
      "       [[[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        ]]]], dtype=float32), array([[0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 42s 836ms/step - loss: 11.1282 - accuracy: 0.2449 - val_loss: 12.8410 - val_accuracy: 0.1950\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 41s 826ms/step - loss: 12.7492 - accuracy: 0.1936 - val_loss: 12.8410 - val_accuracy: 0.1950\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 42s 848ms/step - loss: 12.7492 - accuracy: 0.1936 - val_loss: 12.8410 - val_accuracy: 0.1950\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 41s 837ms/step - loss: 12.7492 - accuracy: 0.1936 - val_loss: 12.8410 - val_accuracy: 0.1950\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 42s 866ms/step - loss: 12.7492 - accuracy: 0.1936 - val_loss: 12.8410 - val_accuracy: 0.1950\n",
      "Epoch 6/10\n",
      "28/49 [================>.............] - ETA: 21s - loss: 12.4915 - accuracy: 0.1857"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(6, activation=None)  # Binary classification output\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
